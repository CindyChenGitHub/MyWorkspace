{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 32-bit"
  },
  "interpreter": {
   "hash": "354ddf9d7eeffc9afadb00fc527a7a2c5e84bfe125ae67ba9b562508e2857d4c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ETL phase 3: Data Cleansing\r\n",
    "<img src=\"data-prep-kdd-process-crisp-dm.jpg\" width=\"1000\" height=\"600\">\r\n",
    "<h2><center>Data preparation in both the KDD Process (left) and the CRISP-DM model (right).</center></h2>\r\n",
    "<img src=\"kdd.gif\" width=\"800\" height=\"500\">\r\n",
    "<h2><center>KDD - Knowledge Discovery in Databases</center></h2>\r\n",
    "<img src=\"CRISP-DM_Process_Diagram.png\" width=\"500\" height=\"300\">\r\n",
    "<h2><center>CRISP - Cross-industry standard process for data mining</center></h2>\r\n",
    "\r\n",
    "Data cleansing consists of following 3 processes usally<br> \r\n",
    "* Missing Values\r\n",
    "* Outlier Values\r\n",
    "* Duplidated values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. import necessary modules（Optional）"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#==============================================\r\n",
    "#=== Mothod 1: Install and import necessary modules\r\n",
    "#==============================================\r\n",
    "from tool_import_modules import *\r\n",
    "modules = ['os', 'pandas', 'pyodbc', 'numpy', 'glob', 'seaborn', 'matplotlib', 'logging', 'time', 'xlwt', 'xlrd', 'openpyxl']\r\n",
    "for module in modules:\r\n",
    "    import_neccessary_modules(module)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#==============================================\r\n",
    "#=== Mothod 2: Import modules directly\r\n",
    "#==============================================\r\n",
    "import os\r\n",
    "import pandas\r\n",
    "import pyodbc\r\n",
    "import numpy as np\r\n",
    "import glob\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import logging\r\n",
    "import time\r\n",
    "import xlwt\r\n",
    "import xlrd\r\n",
    "import openpyxl as xl"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Set path, config, and connection（Optional）"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Set path\r\n",
    "my_path = r\"C:\\MyDataFiles\\Data_JayCoop_202109\"\r\n",
    "my_path_DB = my_path + \"\\DB\"\r\n",
    "my_path_DW = my_path + \"\\DW\"\r\n",
    "my_path_cleaned = my_path + \"\\cleaned\"\r\n",
    "directors =  [my_path_DB, my_path_DW, my_path_cleaned]\r\n",
    "# Set file names\r\n",
    "log_fileName = time.strftime(\"%Y%m%d\") + '_DB.log'\r\n",
    "audit_fileName = time.strftime(\"%Y%m%d\") + '_DB_audit.xls'\r\n",
    "audit_fullPath = os.path.join(my_path, audit_fileName)\r\n",
    "# Set log file\r\n",
    "os.chdir(my_path)\r\n",
    "LOG = logging.getLogger(log_fileName)\r\n",
    "LOG.setLevel(logging.DEBUG)\r\n",
    "# Create file handler which logs even debug messages\r\n",
    "fh = logging.FileHandler(log_fileName, 'w') # 'w'-overwrite; 'a'-append\r\n",
    "fh.setLevel(logging.INFO)\r\n",
    "# Create console handler with a higher log level\r\n",
    "ch = logging.StreamHandler()\r\n",
    "ch.setLevel(logging.DEBUG)\r\n",
    "# Create formatter and add it to the handlers\r\n",
    "formatter = logging.Formatter('%(asctime)s : [%(levelname)s] %(message)s')\r\n",
    "fh.setFormatter(formatter)\r\n",
    "ch.setFormatter(formatter)\r\n",
    "# Add the handlers to the logger\r\n",
    "LOG.addHandler(fh)\r\n",
    "LOG.addHandler(ch)\r\n",
    "# Check path\r\n",
    "if not os.path.exists(my_path):\r\n",
    "    os.makedirs(my_path)\r\n",
    "    LOG.info(\"Directory created: \" + my_path)\r\n",
    "# Check directors\r\n",
    "for director in directors:\r\n",
    "    if not os.path.exists(director):\r\n",
    "        os.makedirs(director)\r\n",
    "        LOG.debug('\\nDirectory created: ' + director)\r\n",
    "# Check auditExcel\r\n",
    "if not os.path.isfile(audit_fullPath):\r\n",
    "    auditExcel = xlwt.Workbook()\r\n",
    "    sheet1 = auditExcel.add_sheet('Files')\r\n",
    "    adtExcSh1Row = 0\r\n",
    "    sheet1.write(0, 0, 'File')\r\n",
    "    sheet1.write(0, 1, 'CreatedTime')\r\n",
    "    sheet1.write(0, 2, 'Path')\r\n",
    "    sheet2 = auditExcel.add_sheet('Cleansing')\r\n",
    "    adtExcSh2Row = 0\r\n",
    "    sheet2.write(0, 0, 'Database')\r\n",
    "    sheet2.write(0, 1, 'Table')\r\n",
    "    sheet2.write(0, 2, 'Column')\r\n",
    "    sheet2.write(0, 3, 'Value')\r\n",
    "    sheet2.write(0, 4, 'issue')\r\n",
    "else:\r\n",
    "    open(audit_fullPath, \"a\")\r\n",
    "    #auditExcel = pandas.ExcelFile(audit_fileName)\r\n",
    "    # auditExcel = xl.load_workbook(audit_fullPath)\r\n",
    "    #auditExcel = pandas.ExcelFile(audit_fullPath)\r\n",
    "    auditExcel = xlrd.open_workbook(audit_fileName)\r\n",
    "    sheet2 = auditExcel.sheet_by_name('Cleansing')\r\n",
    "    #row_count = sheet2.max_row\r\n",
    "    #adtExcSh2Row = sheet2.max_row - 1\r\n",
    "    adtExcSh2Row = sheet2.nrows\r\n",
    "# Print log header\r\n",
    "LOG.info('==== Cleaning Start ====')\r\n",
    "# Set up SQL Server connector (DATABASE:'0179Orders_Org')\r\n",
    "os.chdir(my_path_DB)\r\n",
    "sql_conn = pyodbc.connect('DRIVER={SQL Server}; SERVER=localhost; DATABASE=0179Orders_Org; UID=sa; PWD=SQLServer2019')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-27 17:40:03,700 : [INFO] ==== Cleaning Start ====\n",
      "2021-09-27 17:40:03,700 : [INFO] ==== Cleaning Start ====\n",
      "2021-09-27 17:40:03,700 : [INFO] ==== Cleaning Start ====\n",
      "2021-09-27 17:40:03,700 : [INFO] ==== Cleaning Start ====\n",
      "2021-09-27 17:40:03,700 : [INFO] ==== Cleaning Start ====\n",
      "[2021-09-27 17:40:03,700] [INFO] [20210927_DB.log] - ==== Cleaning Start ====\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Data overview"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Get table name list\r\n",
    "os.chdir(my_path_DB)\r\n",
    "DB_files=os.listdir()\r\n",
    "\r\n",
    "#LOG.debug(DB_files)\r\n",
    "createVar = locals()\r\n",
    "#print(createVar)\r\n",
    "for i in DB_files:\r\n",
    "    if i.endswith(\"csv\"):\r\n",
    "        tableName = i.split('.')[0]\r\n",
    "        #print(createVar[tableName])\r\n",
    "        createVar[tableName] = pandas.read_csv(i)\r\n",
    "        #print(createVar[tableName])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\bluea\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3441: DtypeWarning: Columns (12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27568/2583101959.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtableName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#print(createVar[tableName])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mcreateVar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtableName\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;31m#print(createVar[tableName])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Cleaning\r\n",
    "os.chdir(my_path_DB)\r\n",
    "#colours = ['#000099', '#ffff00'] # specify the colours - yellow is missing. blue is not missing.\r\n",
    "LOG.info(\"==== Cleaning Start ====\")\r\n",
    "for file in glob.glob(\"*.csv\"):\r\n",
    "    tableName = str(file)[:-4]    \r\n",
    "    cleanFile = tableName + \"_clean.csv\"   \r\n",
    "\r\n",
    "    # Get PK\r\n",
    "    pkNameQuery = \"SELECT Col.Column_Name as PkName from INFORMATION_SCHEMA.TABLE_CONSTRAINTS Tab, INFORMATION_SCHEMA.CONSTRAINT_COLUMN_USAGE Col WHERE Col.Constraint_Name = Tab.Constraint_Name AND Col.Table_Name = Tab.Table_Name AND Constraint_Type = 'PRIMARY KEY' AND Col.Table_Name = '\" + tableName +\"'\"\r\n",
    "    pkList = list(pandas.read_sql(pkNameQuery, sql_conn)[\"PkName\"])\r\n",
    "\r\n",
    "    # Get data\r\n",
    "    df = pandas.read_csv(file, index_col = pkList)\r\n",
    "    size_org = df.shape[0]\r\n",
    "    cols = df.columns\r\n",
    "    LOG.info('From: ' + file + ' - size' + str(df.shape))\r\n",
    "\r\n",
    "    # Drop duplicate\r\n",
    "    df.drop_duplicates(keep=\"first\", inplace=True)\r\n",
    "\r\n",
    "    # Print duplication info\r\n",
    "    size_cleaned = df.shape[0]\r\n",
    "    LOG.info('To    : ' + cleanFile + ' - size' + str(df.shape))\r\n",
    "    num_duplication = size_org - size_cleaned\r\n",
    "    if num_duplication > 0:\r\n",
    "        LOG.info('------ [Duplication] ' + str(num_duplication) + ' records dropped from ' + file)\r\n",
    "        adtExcSh2Row = adtExcSh2Row + 1\r\n",
    "        sheet2.write(adtExcSh2Row, 0, 'Database')\r\n",
    "        sheet2.write(adtExcSh2Row, 1, tableName)\r\n",
    "        sheet2.write(adtExcSh2Row, 2, '-')\r\n",
    "        sheet2.write(adtExcSh2Row, 3, int(num_duplication))\r\n",
    "        sheet2.write(adtExcSh2Row, 4, 'Duplication') \r\n",
    "    # Set numeric columns\r\n",
    "    df_numeric = df.select_dtypes(include=[np.number])\r\n",
    "    numeric_cols = df_numeric.columns.values\r\n",
    "   \r\n",
    "    # Set non numeric columns\r\n",
    "    df_non_numeric = df.select_dtypes(exclude=[np.number])\r\n",
    "    non_numeric_cols = df_non_numeric.columns.values\r\n",
    "\r\n",
    "    for col in df.columns:\r\n",
    "        # cleaning missing\r\n",
    "        missing = df[col].isnull()\r\n",
    "        num_missing = np.sum(missing)\r\n",
    "        pct_missing = np.mean(missing)\r\n",
    "             \r\n",
    "        if num_missing > 0: \r\n",
    "\r\n",
    "            # Print Missing Data Percentage List - % of missing.\r\n",
    "            df['{}_ismissing'.format(col)] = missing\r\n",
    "\r\n",
    "\r\n",
    "            # When numeric, fill with midian value \r\n",
    "            if col in numeric_cols:\r\n",
    "                med = df[col].median()\r\n",
    "                if col == 'NPS':\r\n",
    "                    med = int(med)\r\n",
    "                df[col] = df[col].fillna(med)\r\n",
    "                adtExcSh2Row = adtExcSh2Row + 1\r\n",
    "                sheet2.write(adtExcSh2Row, 0, 'CCBIS')\r\n",
    "                sheet2.write(adtExcSh2Row, 1, tableName)\r\n",
    "                sheet2.write(adtExcSh2Row, 2, col)\r\n",
    "                sheet2.write(adtExcSh2Row, 3, int(num_missing))\r\n",
    "                sheet2.write(adtExcSh2Row, 4, 'Missing')                 \r\n",
    "                LOG.info('------ [Missing] ' + file + ' - \"{}\" - {}%'.format(col, round(pct_missing*100)) + ', ' + str(num_missing) + ' records missed - filling with ' + str(med))\r\n",
    "            # When not numeric, fill with most frequent value     \r\n",
    "            else:\r\n",
    "                top = df[col].describe()['top'] # impute with the most frequent value.\r\n",
    "                df[col] = df[col].fillna(top)\r\n",
    "                adtExcSh2Row = adtExcSh2Row + 1\r\n",
    "                sheet2.write(adtExcSh2Row, 0, 'CCBIS')\r\n",
    "                sheet2.write(adtExcSh2Row, 1, tableName)\r\n",
    "                sheet2.write(adtExcSh2Row, 2, col)\r\n",
    "                sheet2.write(adtExcSh2Row, 3, int(num_missing))\r\n",
    "                sheet2.write(adtExcSh2Row, 4, 'Missing')   \r\n",
    "                LOG.info('------ [Missing] ' + file + ' - \"{}\" - {}%'.format(col, round(pct_missing*100)) + ', ' + str(num_missing) + ' records missed - filling with \"' + top + '\"')\r\n",
    "\r\n",
    "        # cleaning outliner\r\n",
    "        #df.boxplot(column=col)\r\n",
    "\r\n",
    "    # write to the new csf in 'cleaned' director\r\n",
    "    try:\r\n",
    "        df.to_csv(my_path_cleaned + \"/\" + cleanFile)\r\n",
    "        adtExcSh1Row = adtExcSh1Row + 1\r\n",
    "        sheet1.write(adtExcSh1Row, 0, str(cleanFile))\r\n",
    "        sheet1.write(adtExcSh1Row, 1, time.asctime())\r\n",
    "        sheet1.write(adtExcSh1Row, 2, my_path_cleaned)\r\n",
    "    except:\r\n",
    "        tb = sys.exc_info()[2]\r\n",
    "        LOG.warn('**** File did NOT update successfully. Please try again after make sure file is not opened and have pomission to write. - ' + my_path_cleaned + \"/\" + cleanFile)\r\n",
    "        continue\r\n",
    "    \r\n",
    "    os.chdir(my_path_DB)\r\n",
    "\r\n",
    "auditExcel.save(audit_fullPath)\r\n",
    "LOG.info('Cleaning Completed Successfully - ' + str(len(os.listdir(my_path_cleaned))) + ' files created in ' + my_path_cleaned)    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-27 17:40:18,289 : [INFO] ==== Cleaning Start ====\n",
      "2021-09-27 17:40:18,289 : [INFO] ==== Cleaning Start ====\n",
      "2021-09-27 17:40:18,289 : [INFO] ==== Cleaning Start ====\n",
      "2021-09-27 17:40:18,289 : [INFO] ==== Cleaning Start ====\n",
      "2021-09-27 17:40:18,289 : [INFO] ==== Cleaning Start ====\n",
      "[2021-09-27 17:40:18,289] [INFO] [20210927_DB.log] - ==== Cleaning Start ====\n",
      "2021-09-27 17:40:18,837 : [INFO] From: Categories.csv - size(1009, 3)\n",
      "2021-09-27 17:40:18,837 : [INFO] From: Categories.csv - size(1009, 3)\n",
      "2021-09-27 17:40:18,837 : [INFO] From: Categories.csv - size(1009, 3)\n",
      "2021-09-27 17:40:18,837 : [INFO] From: Categories.csv - size(1009, 3)\n",
      "2021-09-27 17:40:18,837 : [INFO] From: Categories.csv - size(1009, 3)\n",
      "[2021-09-27 17:40:18,837] [INFO] [20210927_DB.log] - From: Categories.csv - size(1009, 3)\n",
      "2021-09-27 17:40:18,911 : [INFO] To    : Categories_clean.csv - size(906, 3)\n",
      "2021-09-27 17:40:18,911 : [INFO] To    : Categories_clean.csv - size(906, 3)\n",
      "2021-09-27 17:40:18,911 : [INFO] To    : Categories_clean.csv - size(906, 3)\n",
      "2021-09-27 17:40:18,911 : [INFO] To    : Categories_clean.csv - size(906, 3)\n",
      "2021-09-27 17:40:18,911 : [INFO] To    : Categories_clean.csv - size(906, 3)\n",
      "[2021-09-27 17:40:18,911] [INFO] [20210927_DB.log] - To    : Categories_clean.csv - size(906, 3)\n",
      "2021-09-27 17:40:18,919 : [INFO] ------ [Duplication] 103 records dropped from Categories.csv\n",
      "2021-09-27 17:40:18,919 : [INFO] ------ [Duplication] 103 records dropped from Categories.csv\n",
      "2021-09-27 17:40:18,919 : [INFO] ------ [Duplication] 103 records dropped from Categories.csv\n",
      "2021-09-27 17:40:18,919 : [INFO] ------ [Duplication] 103 records dropped from Categories.csv\n",
      "2021-09-27 17:40:18,919 : [INFO] ------ [Duplication] 103 records dropped from Categories.csv\n",
      "[2021-09-27 17:40:18,919] [INFO] [20210927_DB.log] - ------ [Duplication] 103 records dropped from Categories.csv\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Sheet' object has no attribute 'write'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25308/1343250293.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mLOG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'------ [Duplication] '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_duplication\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' records dropped from '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0madtExcSh2Row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madtExcSh2Row\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0msheet2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madtExcSh2Row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Database'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0msheet2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madtExcSh2Row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0msheet2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madtExcSh2Row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sheet' object has no attribute 'write'"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}