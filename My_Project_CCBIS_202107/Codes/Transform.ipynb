{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 32-bit"
  },
  "interpreter": {
   "hash": "354ddf9d7eeffc9afadb00fc527a7a2c5e84bfe125ae67ba9b562508e2857d4c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "#=== Install and import necessary modules\n",
    "#=============================================================================\n",
    "from import_neccessary_modules import *\n",
    "modules = ['os', 'pandas', 'pyodbc', 'numpy', 'glob', 'csv', 'tarfile', 'docker']\n",
    "for module in modules:\n",
    "    import_neccessary_modules(module)\n",
    "\n",
    "import numpy as np\n",
    "import docker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import tarfile\n",
    "import docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(data, oldnames, newname):\n",
    "    if type(oldnames) == str: # Input can be a string or list of strings\n",
    "        oldnames = [oldnames] # When renaming multiple columns\n",
    "        newname = [newname] # Make sure you pass the corresponding list of new names\n",
    "    i = 0\n",
    "    for name in oldnames:\n",
    "        oldvar = [c for c in data.columns if name in c]\n",
    "        if len(oldvar) == 0:\n",
    "            raise ValueError(\"Sorry, couldn't find that column in the dataset\")\n",
    "        if len(oldvar) > 1: # Doesn't have to be an exact match\n",
    "            print(\"Found multiple columns that matched \" + str(name) + \": \")\n",
    "            for c in oldvar:\n",
    "                print(str(oldvar.index(c)) + \": \" + str(c))\n",
    "            ind = input('Please enter the index of the column you would like to rename: ')\n",
    "            oldvar = oldvar[int(ind)]\n",
    "        if len(oldvar) == 1:\n",
    "            oldvar = oldvar[0]\n",
    "        data = data.rename(columns = {oldvar : newname[i]})\n",
    "        i += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "my_path = r\"C:\\MyDataFiles\\Data_CCBIS_202107\"\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "    print(\"Directory created: \" + my_path)\n",
    "# Create data directors\n",
    "my_path_CCBIS = my_path + \"\\CCBIS\"\n",
    "my_path_CCBISDW = my_path + \"\\CCBISDW\"\n",
    "my_path_cleaned = my_path + \"\\cleaned\"\n",
    "directors =  [my_path_CCBIS, my_path_CCBISDW, my_path_cleaned]\n",
    "for director in directors:\n",
    "    if not os.path.exists(director):\n",
    "        os.makedirs(director)\n",
    "        print('Directory created: ' + director)\n",
    "# Set up SQL Server connector\n",
    "sql_conn = pyodbc.connect('DRIVER={SQL Server}; SERVER=localhost; DATABASE=CCBIS; UID=sa; PWD=SQLServer2019') \n",
    "sql_dw_conn = pyodbc.connect('DRIVER={SQL Server}; SERVER=localhost; DATABASE=CCBISDW; UID=sa; PWD=SQLServer2019')   \n",
    "# Set up transform tables\n",
    "mergeTables = {\n",
    "    1: {\n",
    "        \"mergeFrom\":    \"DimGeography_clean.csv\",\n",
    "        \"mergeTo\":      \"DimCustomer_clean.csv\",\n",
    "        \"dw_new\":       \"DimCustomer_DW.csv\",\n",
    "        \"mergeBy\":      \"GeographyKey\",\n",
    "        \"tableName\":    \"DimCustomer\",\n",
    "        \"column_2\":     \"no\",\n",
    "        \"column_new\":   \"no\"  \n",
    "    },\n",
    "    2: {\n",
    "        \"mergeFrom\":    \"DimProductGroup_clean.csv\",\n",
    "        \"mergeTo\":      \"DimProduct_clean.csv\",\n",
    "        \"dw_new\":       \"DimProduct_DW.csv\",\n",
    "        \"mergeBy\":      \"ProductGroup_Key\",\n",
    "        \"tableName\":    \"DimProduct\",\n",
    "        \"column_2\":     \"Name_2\",   \n",
    "        \"column_new\":   \"ProductGroup\"\n",
    "    },\n",
    "    # 3: {\n",
    "    #     \"mergeFrom\":    \"CDR_clean.csv\",\n",
    "    #     \"mergeTo\":      \"\",\n",
    "    #     \"d# new\":       \"FactCDR_DW.csv\",\n",
    "    #     \"mer# By\":      \"\",\n",
    "    #     \"table# me\":    \"FactCDR\",\n",
    "    #     \"column_# :     \"\",   \n",
    "    #     \"column_ne# :   \"\"\n",
    "    # },\n",
    "    4: {\n",
    "        \"mergeFrom\":    \"DimAgent_clean.csv\",\n",
    "        \"mergeTo\":      \"\",\n",
    "        \"dw_new\":       \"DimAgent_DW.csv\",\n",
    "        \"mergeBy\":      \"\",\n",
    "        \"tableName\":    \"DimAgent\",\n",
    "        \"column_2\":     \"\",   \n",
    "        \"column_new\":   \"\"\n",
    "    },\n",
    "    5: {\n",
    "        \"mergeFrom\":    \"DimHandleType_clean.csv\",\n",
    "        \"mergeTo\":      \"\",\n",
    "        \"dw_new\":       \"DimHandleType_DW.csv\",\n",
    "        \"mergeBy\":      \"\",\n",
    "        \"tableName\":    \"DimHandleType\",\n",
    "        \"column_2\":     \"\",   \n",
    "        \"column_new\":   \"\"\n",
    "    },\n",
    "    6: {\n",
    "        \"mergeFrom\":    \"DimServiceType_clean.csv\",\n",
    "        \"mergeTo\":      \"\",\n",
    "        \"dw_new\":       \"DimServiceType_DW.csv\",\n",
    "        \"mergeBy\":      \"\",\n",
    "        \"tableName\":    \"DimServiceType\",\n",
    "        \"column_2\":     \"\",   \n",
    "        \"column_new\":   \"\"\n",
    "    },\n",
    "    7: {\n",
    "        \"mergeFrom\":    \"DimSeverifyType_clean.csv\",\n",
    "        \"mergeTo\":      \"\",\n",
    "        \"dw_new\":       \"DimSeverifyType_DW.csv\",\n",
    "        \"mergeBy\":      \"\",\n",
    "        \"tableName\":    \"DimSeverifyType\",\n",
    "        \"column_2\":     \"\",   \n",
    "        \"column_new\":   \"\"\n",
    "    }\n",
    "}\n",
    "transTables = {\n",
    "    \"CDR_clean.csv\":                \"FactCDR\",\n",
    "    \"DimAgent_clean.csv\":           \"DimAgent\",\n",
    "    \"DimHandleType_clean.csv\":      \"DimHandleType\",\n",
    "    \"DimServiceType_clean.csv\":     \"DimServiceType\",\n",
    "    \"DimSeverifyType_clean.csv\":    \"DimSeverifyType\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_138736/3726573332.py, line 32)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\bluea\\AppData\\Local\\Temp/ipykernel_138736/3726573332.py\"\u001b[1;36m, line \u001b[1;32m32\u001b[0m\n\u001b[1;33m    col = int(col[])\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#=============================================================================\n",
    "#=== Transform - Merge table\n",
    "#=============================================================================\n",
    "\n",
    "print('==== Transform: Merge tables ====')\n",
    "os.chdir(my_path_cleaned)\n",
    "for i in mergeTables.values():\n",
    "    mergeFrom = i.get(\"mergeFrom\")\n",
    "    mergeTo = i.get(\"mergeTo\")\n",
    "    dw_new = i.get(\"dw_new\")\n",
    "    mergeBy = i.get(\"mergeBy\")\n",
    "    tableName = i.get(\"tableName\")\n",
    "\n",
    "    # Get data\n",
    "    df_from = pandas.read_csv(mergeFrom)\n",
    "    size_org = df_from.shape[0]\n",
    "    print('\\nFrom: ' + mergeFrom + str(df_from.shape))\n",
    "    print(df_from.columns)\n",
    "    if tableName == 'DimCustomer':\n",
    "        for col in df_from.columns:\n",
    "            if col == 'NameStyle':\n",
    "                #df.rename(columns={col:'Gold'+col[4:]}, inplace=True)\n",
    "                #pandas.to_numeric(col, downcast=\"integer\")\n",
    "                col = col.astype(int)\n",
    "        #df_from = pd.DataFrame.astype(\"NameStyle\":int)\n",
    "\n",
    "    # if tableName == 'FactCDR':\n",
    "    #     for col in df_from.columns:\n",
    "    #         if col == 'NPS':\n",
    "    #             #df.rename(columns={col:'Gold'+col[4:]}, inplace=True)\n",
    "    #             #pandas.to_numeric(col, downcast=\"integer\")\n",
    "    #             col = int(col[])\n",
    "    #     #df_from = df_from.astype(\"NPS\":int)\n",
    "    if mergeTo ==\"\":\n",
    "        df_new = df_from\n",
    "    else:\n",
    "        df_to = pandas.read_csv(mergeTo)\n",
    "        size_org = df_to.shape[0]\n",
    "        print('To: ' + mergeTo + str(df_to.shape))\n",
    "        print(df_to.columns)\n",
    "\n",
    "        # Merge two tables\n",
    "        df_new = pandas.merge(df_to, df_from, how = 'left', on = mergeBy, suffixes=('', '_2'))\n",
    "    \n",
    "        #df_new.head()\n",
    "\n",
    "        # Deal with duplicated column names\n",
    "    \n",
    "        if mergeFrom == 'DimProductGroup_clean.csv':\n",
    "            df_new = rename(df_new, ['Name_2'], ['ProductGroup'])\n",
    "        #    df_new = rename(df_new, column_2, column_new)\n",
    "    toNew_column_name = list(df_new.columns)\n",
    "    #for column in toNew_column_name:\n",
    "    #    if column in column_2:\n",
    "    \n",
    "    print(len(toNew_column_name))    \n",
    "    print(toNew_column_name)\n",
    " \n",
    "    # Check destination DW columns\n",
    "    query = \"SELECT * FROM [dbo].[\" + tableName + \"]\"\n",
    "    #print(query)\n",
    "    df_dw = pandas.read_sql(query, sql_dw_conn)\n",
    "    dw_column_name = list(df_dw.columns)\n",
    "    print(len(dw_column_name))    \n",
    "    print(dw_column_name)\n",
    "\n",
    "    for column in toNew_column_name:\n",
    "        if column not in dw_column_name:\n",
    "            print(column)\n",
    "            df_new.drop(column, axis = 1, inplace = True)\n",
    "    #print(df_new.head())\n",
    "\n",
    "    # Rearragnge Columns\n",
    "    df_new = df_new[dw_column_name]\n",
    "    print(len(df_new.head()))\n",
    "    print(df_new.head())\n",
    "\n",
    "    # Export to DW .csv\n",
    "    os.chdir(my_path_CCBISDW)\n",
    "    df_new.to_csv(dw_new, index = False)\n",
    "    df_new = pandas.read_csv(dw_new)\n",
    "    size_org = df_new.shape[0]\n",
    "    print('\\nCreate DW file: ' + dw_new + str(df_new.shape))\n",
    "    os.chdir(my_path_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==== Validation Tables ====\n",
      "\n",
      "From: DimCustomer_DW.csv(18484, 37)\n",
      "\n",
      "From: DimProduct_DW.csv(25, 3)\n"
     ]
    }
   ],
   "source": [
    "#=============================================================================\n",
    "#=== Validation\n",
    "#=============================================================================\n",
    "\n",
    "os.chdir(my_path_CCBISDW)\n",
    "print(\"==== Validation Tables ====\")\n",
    "\n",
    "for file in glob.glob(\"*.csv\"):\n",
    "    # Get table info\n",
    "    tableName = str(file)[:-4]\n",
    "    pkNameQuery = \"SELECT Col.Column_Name as PkName from INFORMATION_SCHEMA.TABLE_CONSTRAINTS Tab, INFORMATION_SCHEMA.CONSTRAINT_COLUMN_USAGE Col WHERE Col.Constraint_Name = Tab.Constraint_Name AND Col.Table_Name = Tab.Table_Name AND Constraint_Type = 'PRIMARY KEY' AND Col.Table_Name = '\" + tableName +\"'\"\n",
    "    pkList = list(pandas.read_sql(pkNameQuery, sql_conn)[\"PkName\"])\n",
    "\n",
    "    # Get data\n",
    "    df = pandas.read_csv(file, index_col = pkList)\n",
    "    size_org = df.shape[0]\n",
    "    print('\\nFrom: ' + file + str(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "#=== SQL Bulk Insert Procedue\n",
    "#=============================================================================\n",
    "class c_bulk_insert:\n",
    "    def __init__(self, csv_file_nm, db_nm, db_table_nm):\n",
    "        # Connect to the database, perform the insert, and update the log table.\n",
    "        \n",
    "        conn = self.connect_db()\n",
    "        self.insert_data(conn, csv_file_nm, db_table_nm)\n",
    "        conn.close\n",
    "    def connect_db(self):\n",
    "        # Connect to the server and database with Windows authentication.\n",
    "        # conn_string = 'DRIVER={SQL Server}; SERVER = localhost; DATABASE=' + db_nm + '; UID=sa; PWD=SQLServer2019; Trusted_Connection=yes'\n",
    "        conn = pyodbc.connect('DRIVER={SQL Server}; SERVER=localhost; DATABASE=CCBISDW; UID=sa; PWD=SQLServer2019') \n",
    "        # conn = pyodbc.connect(conn_string)\n",
    "        return conn\n",
    "    def insert_data(self, conn, csv_file_nm, db_table_nm):\n",
    "        # Insert the data from the CSV file into the database table.\n",
    "        # Assemble the BULK INSERT query. Be sure to skip the header row by specifying FIRSTROW = 2.\n",
    "        qry = \"BULK INSERT \" + db_table_nm + \" FROM '\" + csv_file_nm + \"' WITH (FORMAT = 'CSV', FIRSTROW = 2)\"\n",
    "        # Execute the query\n",
    "        cursor = conn.cursor()\n",
    "        success = cursor.execute(qry)\n",
    "        conn.commit()\n",
    "        cursor.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "b'hello world\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import docker\n",
    "client = docker.from_env()\n",
    "client.containers.run(\"alpine\", \"echo hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "#=== Copy .csv to Docker Container Procedue\n",
    "#=============================================================================\n",
    "import docker\n",
    "client = docker.from_env()\n",
    "# src - from file name (in local), shall be an absolute path of fromFile\n",
    "# dst - to dir (in docker container)\n",
    "def copy_to_container(src, dst):\n",
    "    name, dst = dst.split(':')\n",
    "    container = client.containers.get(name)\n",
    "\n",
    "    os.chdir(os.path.dirname(src))\n",
    "    srcname = os.path.basename(src)\n",
    "    tar = tarfile.open(src + '.tar', mode='w')\n",
    "    try:\n",
    "        tar.add(srcname)\n",
    "    finally:\n",
    "        tar.close()\n",
    "\n",
    "    data = open(src + '.tar', 'rb').read()\n",
    "    container.put_archive(os.path.dirname(dst), data)\n",
    "# To use\n",
    "# copy_to_container(\"C:\\MyDataFiles\\Data_CCBIS_202107\\CCBISDW\\DimCustomer_DW.csv\", 'SQL_Server_2019:/var/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DimAgent\n",
      "C:\\MyDataFiles\\Data_CCBIS_202107\\CCBISDW\\DimAgent_DW.csv\n",
      "SQL_Server_2019:/var/tmp\n",
      "DimCustomer\n",
      "C:\\MyDataFiles\\Data_CCBIS_202107\\CCBISDW\\DimCustomer_DW.csv\n",
      "SQL_Server_2019:/var/tmp\n",
      "DimHandleType\n",
      "C:\\MyDataFiles\\Data_CCBIS_202107\\CCBISDW\\DimHandleType_DW.csv\n",
      "SQL_Server_2019:/var/tmp\n",
      "DimProduct\n",
      "C:\\MyDataFiles\\Data_CCBIS_202107\\CCBISDW\\DimProduct_DW.csv\n",
      "SQL_Server_2019:/var/tmp\n",
      "DimServiceType\n",
      "C:\\MyDataFiles\\Data_CCBIS_202107\\CCBISDW\\DimServiceType_DW.csv\n",
      "SQL_Server_2019:/var/tmp\n",
      "DimSeverifyType\n",
      "C:\\MyDataFiles\\Data_CCBIS_202107\\CCBISDW\\DimSeverifyType_DW.csv\n",
      "SQL_Server_2019:/var/tmp\n",
      "FactCDR\n",
      "C:\\MyDataFiles\\Data_CCBIS_202107\\CCBISDW\\FactCDR_DW.csv\n",
      "SQL_Server_2019:/var/tmp\n"
     ]
    }
   ],
   "source": [
    "# Copy .csv to Docker Container\n",
    "for file in glob.glob(\"*.csv\"):\n",
    "    os.chdir(my_path_CCBISDW)\n",
    "    fileName = os.path.join(my_path_CCBISDW, file)\n",
    "    tableName = str(file)[:-7]\n",
    "    toContainerDir = 'SQL_Server_2019:/var/tmp'\n",
    "    print(tableName)\n",
    "    print(fileName)\n",
    "    print(toContainerDir)\n",
    "    copy_to_container(fileName, toContainerDir)\n",
    "    #bulk_insert = c_bulk_insert(fileName, 'CCBISDW', tableName) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==== Load to WD ====\nDimAgent\n\\var\\tmp\\DimAgent_DW.csv\nuse CCBISDW BULK INSERT DimAgent FROM 'SQL_Server_2019:/var/tmp/DimAgent_DW.csv' \nselect top (10) * from DimAgent\nDimCustomer\n\\var\\tmp\\DimCustomer_DW.csv\nuse CCBISDW BULK INSERT DimAgent FROM 'SQL_Server_2019:/var/tmp/DimAgent_DW.csv' \nselect top (10) * from DimCustomer\nDimHandleType\n\\var\\tmp\\DimHandleType_DW.csv\nuse CCBISDW BULK INSERT DimAgent FROM 'SQL_Server_2019:/var/tmp/DimAgent_DW.csv' \nselect top (10) * from DimHandleType\nDimProduct\n\\var\\tmp\\DimProduct_DW.csv\nuse CCBISDW BULK INSERT DimAgent FROM 'SQL_Server_2019:/var/tmp/DimAgent_DW.csv' \nselect top (10) * from DimProduct\nDimServiceType\n\\var\\tmp\\DimServiceType_DW.csv\nuse CCBISDW BULK INSERT DimAgent FROM 'SQL_Server_2019:/var/tmp/DimAgent_DW.csv' \nselect top (10) * from DimServiceType\nDimSeverifyType\n\\var\\tmp\\DimSeverifyType_DW.csv\nuse CCBISDW BULK INSERT DimAgent FROM 'SQL_Server_2019:/var/tmp/DimAgent_DW.csv' \nselect top (10) * from DimSeverifyType\nFactCDR\n\\var\\tmp\\FactCDR_DW.csv\nuse CCBISDW BULK INSERT DimAgent FROM 'SQL_Server_2019:/var/tmp/DimAgent_DW.csv' \nselect top (10) * from FactCDR\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function Cursor.close>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "#=============================================================================\n",
    "#=== Load CSV to DW\n",
    "#=============================================================================\n",
    "os.chdir(my_path_CCBISDW)\n",
    "print(\"==== Load to WD ====\")\n",
    "# Set up SQL Server connector\n",
    "#sql_dw_conn = pyodbc.connect('DRIVER={SQL Server}; SERVER=localhost; DATABASE=CCBISDW; UID=sa; PWD=SQLServer2019')  \n",
    "\n",
    "cursor = sql_dw_conn.cursor()\n",
    "for file in glob.glob(\"*.csv\"):\n",
    "    # Get table info\n",
    "    tableName = str(file)[:-7]\n",
    "    #fileName = str(os.path.join(my_path_CCBISDW, file))\n",
    "    #fileName = os.path.join(toContainerDir,file)\n",
    "    #toName = 'my-container:/tmp/CCBISDW/DimCustomer_DW.csv'\n",
    "    #copy_to(fileName, toName)\n",
    "    toContainerDir = r'\\var\\tmp'\n",
    "    #fileName = str(toContainerDir + \"\\\" + tableName + '_DW.csv')\n",
    "    #fileName = str(toContainerDir + \"\\\" + tableName + '_DW.csv')\n",
    "    fileName = os.path.join(toContainerDir,(tableName+'_DW.csv'))\n",
    "    #pkNameQuery = \"SELECT Col.Column_Name as PkName from INFORMATION_SCHEMA.TABLE_CONSTRAINTS Tab, INFORMATION_SCHEMA.CONSTRAINT_COLUMN_USAGE Col WHERE Col.Constraint_Name = Tab.Constraint_Name AND Col.Table_Name = Tab.Table_Name AND Constraint_Type = 'PRIMARY KEY' AND Col.Table_Name = '\" + tableName +\"'\"\n",
    "    #pkList = list(pandas.read_sql(pkNameQuery, sql_dw_conn)[\"PkName\"])\n",
    "    print(tableName)\n",
    "    print(fileName)\n",
    "    #from c_bulk_insert import c_bulk_insert\n",
    "    ########bulk_insert = c_bulk_insert(fileName, 'CCBISDW', tableName) \n",
    "    # Get data\n",
    "    #qry = \"SELECT @@SERVERNAME\"\n",
    "    #qry = \"use CCBISDW BULK INSERT dbo.\" + tableName + \" FROM '\" + toContainerDir + \"' WITH (datafiletype = 'char', FIRSTROW = 2, FIELDTERMINATOR ='\\t', ROWTERMINATOR ='\\n')\"\n",
    "    qry = \"use CCBISDW BULK INSERT DimAgent FROM 'SQL_Server_2019:/var/tmp/DimAgent_DW.csv' \"\n",
    "    print(qry)\n",
    "    cursor.execute(qry)\n",
    "    qry = \"select top (10) * from \" + tableName\n",
    "    print(qry)\n",
    "    cursor.execute(qry)\n",
    "    #success = cursor.execute(qry)\n",
    "    #sql_dw_conn.commit()\n",
    "cursor.close\n",
    "    #df = pandas.read_csv(file, index_col = pkList)\n",
    "    #with open (file, 'r') as f:\n",
    "    #    reader = csv.reader(f)\n",
    "    #    columns = next(reader) \n",
    "\n",
    "    # Insert DataFrame to Table\n",
    "    #insert_data(file, sql_dw_conn, fileName, tableName)\n",
    "        #cursor = sql_dw_conn.cursor()\n",
    "        #query = \"INSERT INTO \" + tableName  + 'values ({1})'\n",
    "        #query = query.format(','.join(columns), ','.join('?' * len(columns)))\n",
    "        #query = \"Use CCBISDW bulk insert dbo.\" + tableName + \"From '\" + my_path_CCBISDW + \"\\\" + file + \"' With(Datafile = \"char\", FIRSTROW = 2, FIELDTERMINATOR = \",\", ROWTERMINATOR = \"0x0a\")\" \n",
    "        #for data in reader:\n",
    "        #    cursor.execute(query, data)\n",
    "        #sql_dw_conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}